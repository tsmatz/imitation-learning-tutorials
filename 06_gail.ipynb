{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f35fd98-8613-45a2-959b-e5da21d9d4d3",
   "metadata": {},
   "source": [
    "# Generative Adversarial Imitation Learning (GAIL)\n",
    "\n",
    "Generative Adversarial Imitation Learning (GAIL) is the method inspired by IRL (Inverse Reinforcement Learning) and GANs (Generative Adversarial Networks), in which discriminator network is used to distinguish true data from false data.\n",
    "\n",
    "Unlike previous IRL methods, GAIL constrains the behavior of the agent to be approximately optimal without explicitly recovering the reward's function, i.e, it scales an IRL approach by bypassing intermediate IRL steps (bypassing steps for learning a cost or reward function).<br>\n",
    "It directly extracts a policy from data, **as if it were obtained by reinforcement learning following inverse\n",
    "reinforcement learning**. (RL algorithms discussed in [here](https://github.com/tsmatz/reinforcement-learning-tutorials) can also be applied in GAIL.)\n",
    "\n",
    "Up until now, we assumed that the rewards $\\mathbf{w}$ is linear to the feature of trajectory, $\\verb|reward| = \\mathbf{w}^T \\cdot \\phi(\\tau)$.<br>\n",
    "GAIL, however, doesn't need this constraint, and can then imitate arbitrarily complex expert behaviors.\n",
    "\n",
    "GAIL is one of SOTA (state-of-the-art) imitation learning algorithms.\n",
    "\n",
    "*(back to [index](https://github.com/tsmatz/imitation-learning-tutorials/))*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64bb5d64-6e29-413c-a95b-6b1678611cb0",
   "metadata": {},
   "source": [
    "## Overview of Generative Adversarial Imitation Learning (GAIL) method\n",
    "\n",
    "Let's briefly follow theoretical aspects behind GAIL algorithm along with the original paper [[Ho and Ermon, 2016](https://arxiv.org/pdf/1606.03476)].\n",
    "\n",
    "Maximum Causal Entropy (MCE) IRL is one of successful IRL algorithms.<br>\n",
    "As we have discussed in [MCE IRL example](./04_mce_irl.ipynb), Maximum Causal Entropy IRL finds a cost function (see below note) to maximize the entropy $-H(\\pi)$ and expectation.<br>\n",
    "IRL operation in abstraction can then be written as :\n",
    "\n",
    "$\\displaystyle IRL(\\pi_E) = \\max_{c \\in \\mathcal{C}} \\left( \\min_{\\pi \\in \\Pi} \\left( -H(\\pi) + \\mathbb{E}_{\\pi}[c(s,a)] \\right) - \\mathbb{E}_{\\pi_E}[c(s,a)] \\right)$\n",
    "\n",
    "where $\\mathcal{C}$ is a set of cost functions.\n",
    "\n",
    "> Note : Unlike [MCE IRL example](./04_mce_irl.ipynb), the function $c()$ is not a reward function, but a cost function. The cost function is **minimized (not maximized)** for optimization.\n",
    "\n",
    "To prevent from [overfitting](https://tsmatz.wordpress.com/2017/09/13/overfitting-for-regression-and-deep-learning/) in machine learning problems, we introduce a regularizer, $\\psi : \\mathbb{R}^{\\mathcal{S} \\times \\mathcal{A}} \\to \\mathbb{R} \\cup \\{ \\infty \\}$, and apply in this equation. :\n",
    "\n",
    "$\\displaystyle IRL_{\\psi}(\\pi_E) = \\arg \\max_{c \\in \\mathbb{R}^{\\mathcal{S} \\times \\mathcal{A}}} \\left( -\\psi(c) + \\min_{\\pi \\in \\Pi} \\left( -H(\\pi) + \\mathbb{E}_{\\pi}[c(s,a)] \\right) - \\mathbb{E}_{\\pi_E}[c(s,a)] \\right) \\;\\;\\;\\;\\;\\; (1) $\n",
    "\n",
    "Now we introduce the occupancy measure which is often used in the theory of RL.\n",
    "\n",
    "First, we define the occupancy measure $\\rho_{\\pi}(s, a) : \\mathcal{S} \\times \\mathcal{A} \\to \\mathbb{R}$ for $\\pi$ as :\n",
    "\n",
    "$\\displaystyle \\rho_{\\pi}(s, a) \\coloneqq \\pi(a | s) \\sum_{t=0}^{\\infty} \\gamma^t P(s_t=s|\\pi) $\n",
    "\n",
    "> Note : $P(s_t=s|\\pi)$ is the probability of landing in state $s$.\n",
    "\n",
    "In abstraction, the occupancy measure is the probability of state's occurrence when navigating the environment with policy $\\pi$.\n",
    "\n",
    "When we denote a set of valid occupancy measures $\\{ \\rho_{\\pi} : \\pi \\in \\Pi \\}$ as $\\mathcal{D}$, it's known that there is a one-to-one correspondence between $\\Pi$ and $\\mathcal{D}$. (i.e, When some $\\rho$ is given, the corresponding $\\pi$ to satisfy $\\rho = \\rho_\\pi$ is uniquely determined.)\n",
    "\n",
    "Using occupancy measures, it's known that to find the optimal $\\pi$ under a cost function obtained in (1) is equivalent to :\n",
    "\n",
    "$\\displaystyle RL \\circ IRL_{\\psi}(\\pi_E) = \\arg \\min_{\\pi \\in \\Pi} \\left( -H(\\pi) + \\psi^* \\left( \\rho_{\\pi} - \\rho_{\\pi_E} \\right) \\right) \\;\\;\\;\\;\\;\\; (2)$\n",
    "\n",
    "where $\\psi^* : \\mathbb{R}^{\\mathcal{S} \\times \\mathcal{A}} \\to \\mathbb{R} \\cup \\{ \\infty \\}$ is the [convex conjugate](https://en.wikipedia.org/wiki/Convex_conjugate) of $\\psi$.\n",
    "\n",
    "Especially, when $\\psi$ is constant and $\\pi$ is then obtained by (2), it's known that $\\rho_{\\pi} = \\rho_{\\pi_E}$.\n",
    "\n",
    "> Note : See [original paper](https://arxiv.org/pdf/1606.03476) and its Appendix A.1 for this proof.\n",
    "\n",
    "This implicitly means that $\\psi$-regularized IRL is to seek a policy whose occupancy measure is close to the expert, as measured by the convex function $\\psi^*$. If $\\psi$ is constant, the solution of (2) simply matches occupancy measures with expert at all states and actions.<br>\n",
    "Here I don't go so far into details, but it's also known that the expectation matching with linear cost function (which are discussed in previous examples) is the special case of above (2). (See description about apprenticeship learning in [original paper](https://arxiv.org/pdf/1606.03476) for details.)\n",
    "\n",
    "Now let $\\psi_{GA}()$ be the following regularization function. :\n",
    "\n",
    "$\\displaystyle \\psi_{GA}(c) \\coloneqq \\mathbb{E}_{\\pi_E}[g(c(s,a))] $  when $c \\lt 0$\n",
    "\n",
    "$\\displaystyle \\psi_{GA}(c) \\coloneqq +\\infty $  otherwise\n",
    "\n",
    "where\n",
    "\n",
    "$\\displaystyle g(x) \\coloneqq -x-\\log(1-e^x) $  when $x \\lt 0$\n",
    "\n",
    "$\\displaystyle g(x) \\coloneqq +\\infty $  otherwise\n",
    "\n",
    "This regularization forces that the cost function $c$ is always negative, and penalitizes for small $c$ and large $c$ (close to zero), because the plot of function $g(x)$ is as follows. :\n",
    "\n",
    "<img src=\"./assets/regularization_plot.png\" alt=\"Regularization Plot\" width=\"300\"/>\n",
    "\n",
    "Unlike linear cost functions (or linear reward functions) seen in previous IRL examples, $\\psi_{GA}()$ allows for any cost function, as long as it is negative everywhere, and $RL \\circ IRL_{\\psi}(\\pi_E)$ can be fitted to more complex cost estimation.\n",
    "\n",
    "Under this assumption, it's known that it holds the following equation. (See [original paper](https://arxiv.org/pdf/1606.03476) for the proof.)\n",
    "\n",
    "$\\displaystyle \\psi_{GA}^* (\\rho_{\\pi} - \\rho_{\\pi_E}) = \\max_{D \\in (0,1)^{\\mathcal{S} \\times \\mathcal{A}}} \\left( \\mathbb{E}_{\\pi} [\\log(D(s, a))] + \\mathbb{E}_{\\pi_E} [\\log(1 - D(s, a))] \\right) \\;\\;\\;\\;\\;\\; (3) $\n",
    "\n",
    "where $D$ is discriminative classifier $D : \\mathcal{S} \\times \\mathcal{A} \\to (0,1)$.\n",
    "\n",
    "Now we assume that the learner's policy $\\pi$ is parameterized by $\\theta$ (i.e, $\\pi = \\pi_{\\theta}$) and the discriminator network $D$ is parameterized by $w$ (i.e, $D = D_w$).\n",
    "\n",
    "By above (2) and (3), our algorithm (GAIL) is to alternately update $w$ and $\\theta$ as follows. :\n",
    "\n",
    "- Update $w$ to increase $\\mathbb{E}_{\\pi} [\\log(D(s, a))] + \\mathbb{E}_{\\pi_E} [\\log(1 - D(s, a))]$.\n",
    "- Update $\\theta$ to decrease $\\mathbb{E}_{\\pi} [\\log(D(s, a))] - \\lambda H(\\pi) $ using the updated discriminator $D(s,a)$, where $\\lambda \\geq 0$ is a controlling coefficient.\n",
    "\n",
    "In [original paper](https://arxiv.org/pdf/1606.03476), Adam gradient step is applied to update $w$, and TRPO (trust region policy optimization) algorithm is used to update $\\theta$ in order to avoid unstable policy updates.<br>\n",
    "As you'll see later, we'll however use PPO (proximal policy optimization) instead of TRPO in this example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2887022-1916-46a6-aa93-cf26c322c293",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e686e301-bc83-421a-aff1-6ce67283f4ab",
   "metadata": {},
   "source": [
    "### 1. Restore environment and load expert's data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c802b3dd-8ce5-415a-90b3-7ccbb028d4c4",
   "metadata": {},
   "source": [
    "Before we start, we need to install the required packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e202432-fb4e-4065-b7e3-4e5a3b066d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch numpy matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5260fae-0aea-4077-a1b4-ee164ab296ad",
   "metadata": {},
   "source": [
    "Firstly, I restore GridWorld environment from JSON file. (See [this script](./00_generate_expert_trajectories.ipynb) for generating the same environment.)\n",
    "\n",
    "I note that I'll apply temporal difference (TD) learning (see [here](https://github.com/tsmatz/reinforcement-learning-tutorials/blob/master/03-actor-critic.ipynb)) in this example, and the difference between termination and truncation is important.<br>\n",
    "Thus, by setting ```max_timestep=None```, the truncation is not performed in this environment, and I'll handle the truncation during the training by myself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1ce850d3-00a6-41af-9d3e-84ebbff0132e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import json\n",
    "from gridworld import GridWorld\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "with open(\"gridworld.json\", \"r\") as f:\n",
    "    json_object = json.load(f)\n",
    "    env = GridWorld(**json_object, max_timestep=None, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a00a819e-ebdb-46b2-aff0-fa66c836f6d1",
   "metadata": {},
   "source": [
    "Now I visualize our GridWorld environment.\n",
    "\n",
    "The number in each cell indicates the reward score on this state.<br>\n",
    "The goal state is on the right-bottom corner (in which the reward is ```10.0```), and the initial state is uniformly picked up from the gray-colored cells.<br>\n",
    "If the agent can reach to goal state without losing any rewards, it will get ```10.0``` for total reward.\n",
    "\n",
    "See [Readme.md](https://github.com/tsmatz/imitation-learning-tutorials/blob/master/Readme.md) for details about the game rule of this environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "73d95082-f467-48f2-96a9-1150fe17e810",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table><tr><td>0</td><td>-1</td><td>0</td><td>0</td><td>0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td>0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><td>-1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td>0</td><td>-1</td><td>-1</td><td>0</td><td>0</td><td>0</td><td>-1</td><td>-1</td><td>0</td><td>0</td><td>-1</td><td>-1</td></tr><tr><td>0</td><td>0</td><td>-1</td><td>-1</td><td>0</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>-1</td><td>0</td><td>0</td><td>0</td></tr><tr><td>0</td><td>0</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td>0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>-1</td></tr><tr><td>-1</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>0</td><td>0</td><td>-1</td><td>0</td><td>-1</td><td>-1</td><td>0</td><td>0</td></tr><tr><td>0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>0</td><td>-1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>-1</td></tr><tr><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td>0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td>0</td><td>0</td><td>0</td><td>-1</td><td>0</td><td>-1</td><td>0</td><td>-1</td><td>-1</td></tr><tr><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td>0</td><td>0</td><td>-1</td><td>0</td><td>-1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td>-1</td><td>-1</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td>0</td><td>-1</td><td>0</td><td>0</td><td>0</td><td>-1</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>-1</td><td>-1</td><td>0</td><td>-1</td><td>-1</td><td>-1</td></tr><tr><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td>-1</td><td>-1</td><td>-1</td><td>-1</td><td>0</td><td>0</td><td>-1</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td></tr><tr><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td>0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td></tr><tr><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td></tr><tr><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td></tr><tr><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td>0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td></tr><tr><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td>0</td><td>-1</td><td>0</td><td>-1</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td>-1</td><td>-1</td><td>-1</td><td>-1</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>0</td><td>-1</td></tr><tr><td>0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td>0</td><td>-1</td><td>0</td><td>-1</td><td>0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td>0</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td>-1</td></tr><tr><td>0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>0</td><td>0</td><td>-1</td><td>0</td><td>0</td><td>0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td></tr><tr><td>0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>0</td><td>0</td><td>-1</td><td>0</td><td>0</td><td>0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>0</td></tr><tr><td>0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>0</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td></tr><tr><td>-1</td><td>-1</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td>-1</td></tr><tr><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td>0</td><td>0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td></tr><tr><td>0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td></tr><tr><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td></tr><tr><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td>-1</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td></tr><tr><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td></tr><tr><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td>0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td></tr><tr><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td></tr><tr><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td></tr><tr><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td>-1</td><td>-1</td><td>-1</td><td>-1</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>0</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td></tr><tr><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td></tr><tr><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td>0</td><td>0</td><td>0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td></tr><tr><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td>-1</td><td>0</td><td>-1</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td></tr><tr><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td>0</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td></tr><tr><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td>-1</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td></tr><tr><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td>0</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td></tr><tr><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td></tr><tr><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td></tr><tr><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>0</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td></tr><tr><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td></tr><tr><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td>-1</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td></tr><tr><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td></tr><tr><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td></tr><tr><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td></tr><tr><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td></tr><tr><td>-1</td><td>0</td><td>-1</td><td>-1</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td></tr><tr><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td></tr><tr><td>0</td><td>0</td><td>-1</td><td>-1</td><td>-1</td><td>-1</td><td>-1</td><td>0</td><td>0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td>-1</td><td>0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td></tr><tr><td>-1</td><td>0</td><td>0</td><td>-1</td><td>-1</td><td>0</td><td>-1</td><td>0</td><td>0</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td></tr><tr><td>-1</td><td>0</td><td>0</td><td>0</td><td>-1</td><td>0</td><td>-1</td><td>-1</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">10</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import HTML, display\n",
    "\n",
    "valid_states_all = torch.cat((env.valid_states, torch.tensor([env.grid_size-1,env.grid_size-1]).to(device).unsqueeze(dim=0)))\n",
    "valid_states_all = valid_states_all[:,0] * env.grid_size + valid_states_all[:,1]\n",
    "\n",
    "html_text = \"<table>\"\n",
    "for row in range(env.grid_size):\n",
    "    html_text += \"<tr>\"\n",
    "    for col in range(env.grid_size):\n",
    "        if row*env.grid_size + col in valid_states_all:\n",
    "            html_text += \"<td bgcolor=\\\"gray\\\">\"\n",
    "        else:\n",
    "            html_text += \"<td>\"\n",
    "        html_text += str(env.reward_map[row*env.grid_size+col].tolist())\n",
    "        html_text += \"</td>\"\n",
    "    html_text += \"</tr>\"\n",
    "html_text += \"</table>\"\n",
    "\n",
    "display(HTML(html_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d09eeb91-78f4-42c3-9911-0ae54d357e0f",
   "metadata": {},
   "source": [
    "Load expert's data (demonstrations) which is saved in ```./expert_data``` folder in this repository.\n",
    "\n",
    "> Note : See [this script](./00_generate_expert_trajectories.ipynb) for generating expert dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3adf63df-208b-408b-ac5d-1365309016f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "dest_dir = \"./expert_data\"\n",
    "checkpoint_file = \"ckpt0.pkl\"\n",
    "\n",
    "# load expert data from pickle\n",
    "with open(f\"{dest_dir}/{checkpoint_file}\", \"rb\") as f:\n",
    "    exp_data = pickle.load(f)\n",
    "exp_states = torch.tensor(exp_data[\"states\"]).to(device)\n",
    "exp_actions = torch.tensor(exp_data[\"actions\"]).to(device)\n",
    "timestep_lens = exp_data[\"timestep_lens\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "344eeff1-815d-4d0b-b616-a74c18216d6d",
   "metadata": {},
   "source": [
    "### 2. Create networks\n",
    "\n",
    "Now I define the following policy network $\\pi_{\\theta}$, value network, and discriminator network $D_w$.<br>\n",
    "In TRPO (also, PPO) implementation, not only the policy network, but the value network is also required.\n",
    "\n",
    "- Policy network : It receives one-hot state as input. It then returns the optimal action logits as output.\n",
    "- Value network : It receives one-hot state as input. It then returns a single value as output.\n",
    "- Discriminator network : It receives one-hot state and one-hot action as input. It then returns a value of range (0, 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "11ef1777-36be-48a0-b07a-b856ffa5fc9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "STATE_SIZE = env.grid_size*env.grid_size  # 2500\n",
    "ACTION_SIZE = env.action_size             # 4\n",
    "\n",
    "#\n",
    "# Define model\n",
    "#\n",
    "class PolicyNet(nn.Module):\n",
    "    def __init__(self, hidden_dim=64):\n",
    "        super().__init__()\n",
    "        self.hidden = nn.Linear(STATE_SIZE, hidden_dim)\n",
    "        self.output = nn.Linear(hidden_dim, 4)\n",
    "\n",
    "    def forward(self, s):\n",
    "        outs = self.hidden(s)\n",
    "        outs = F.relu(outs)\n",
    "        logits = self.output(outs)\n",
    "        return logits\n",
    "\n",
    "class ValueNet(nn.Module):\n",
    "    def __init__(self, hidden_dim=64):\n",
    "        super().__init__()\n",
    "        self.hidden = nn.Linear(STATE_SIZE, hidden_dim)\n",
    "        self.output = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, s):\n",
    "        outs = self.hidden(s)\n",
    "        outs = F.relu(outs)\n",
    "        value = self.output(outs)\n",
    "        return value\n",
    "\n",
    "class DiscriminatorNet(nn.Module):\n",
    "    def __init__(self, hidden_dim=64):\n",
    "        super().__init__()\n",
    "        self.hidden1 = nn.Linear(STATE_SIZE + ACTION_SIZE, hidden_dim)\n",
    "        self.hidden2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.get_logits = nn.Linear(hidden_dim, 1)\n",
    "        self.get_sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, s):\n",
    "        outs = self.hidden1(s)\n",
    "        outs = F.relu(outs)\n",
    "        outs = self.hidden2(outs)\n",
    "        outs = F.relu(outs)\n",
    "        logits = self.get_logits(outs)\n",
    "        output = self.get_sigmoid(logits)\n",
    "        return output\n",
    "\n",
    "#\n",
    "# Generate model\n",
    "#\n",
    "policy_func = PolicyNet().to(device)\n",
    "value_func = ValueNet().to(device)\n",
    "discriminator = DiscriminatorNet().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f7b318a-2d41-402e-85a9-d12a22689eee",
   "metadata": {},
   "source": [
    "### 3. Create functions to get learner and expert data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c38c67e1-2d0a-4cf7-9e62-a4c3f034b0d8",
   "metadata": {},
   "source": [
    "Now we create a function to get expert samples.<br>\n",
    "In this example, we get 6000 samples (6000 steps) at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6cddcefb-8961-4646-bc51-cf127505a37e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import itertools\n",
    "\n",
    "num_samples = 6000\n",
    "\n",
    "expert_loader = DataLoader(\n",
    "    list(zip(exp_states, exp_actions)),\n",
    "    batch_size=num_samples,\n",
    "    shuffle=False,\n",
    ")\n",
    "expert_iter = iter(itertools.cycle(expert_loader))\n",
    "\n",
    "def get_data_by_expert(num_samples=num_samples):\n",
    "    data_length = 0\n",
    "    while data_length < num_samples:\n",
    "        states, actions = next(expert_iter)\n",
    "        data_length = len(states)\n",
    "    return states, actions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c3adda4-d147-4234-a80f-34791da215bb",
   "metadata": {},
   "source": [
    "Not only sampling by the expert, but sampling by policy $\\pi$ is also required, because we need the expectation $\\mathbb{E}_{\\pi}(\\cdot)$.\n",
    "\n",
    "In sampling by policy $\\pi$, we need values and advantages for applying TRPO (or PPO) algorithm.\n",
    "\n",
    "> Note : See [here](https://github.com/tsmatz/reinforcement-learning-tutorials/blob/master/03-actor-critic.ipynb) for theoretical aspects of values and advantages in RL.\n",
    "\n",
    "In this example, I'll use TD (temporal difference) and GAE (generalized advantage estimation) for getting values and advantages in TRPO (or PPO).<br>\n",
    "Now I briefly summarize TD and GAE as follows.\n",
    "\n",
    "Suppose, $V(\\cdot)$ is the value network.<br>\n",
    "In TD (temporal difference), we get the value at time-step $t$ by $r_t + \\gamma V(s_{t+1})$, where $r_t$ is a real score at $t$. (I note that $r_t = \\log(D(s_t, a_t))$ in this example.)<br>\n",
    "We then get value's difference (between value and estimated value) $\\delta_t^V$ by :\n",
    "\n",
    "$\\displaystyle \\delta_t^V \\coloneqq r_t + \\gamma V(s_{t+1}) - V(s_t)$\n",
    "\n",
    "In [GAE (generalized advantage estimation)](https://arxiv.org/pdf/1506.02438), the advantage at $t$ is obtained by :\n",
    "\n",
    "$\\displaystyle \\hat{A}_t^{\\verb|GAE|} \\coloneqq \\sum_{l=0}^{\\infty} (\\gamma \\lambda_{\\verb|GAE|})^l \\delta_{t+l}^V$\n",
    "\n",
    "where $\\lambda_{\\verb|GAE|} \\in [0, 1]$ is a controlling parameter (between bias and variance) in GAE.\n",
    "\n",
    "When you set $\\lambda_{\\verb|GAE|}=1.0$, then $\\hat{A}_t^{\\verb|GAE|}$ is just a diffrence of values without generalization as follows. (See [original paper](https://arxiv.org/pdf/1506.02438) for details.)\n",
    "\n",
    "$\\displaystyle \\sum_{l=0}^{k - 1} \\gamma^l \\delta_{t+l}^V = -V(s_t) + r_t + \\gamma r_{t+1} + \\cdots + \\gamma^{k-1} r_{t+k-1} + \\gamma^k V(s_{t+k}) $\n",
    "\n",
    "It's worth noting that the rewards are never used in GAIL algorithm (because it's imitation learning !), but **here we use reward to evaluate how the agent is learned** in this example. (You won't be able to use a reward in practical training.)\n",
    "\n",
    "> Note : As I have mentioned above, the truncation is not performed in the environemnt. Instead, this function handles the truncation as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "793d5200-61dc-45de-b94c-119f4cae968a",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_TIMESTEP = 200\n",
    "\n",
    "gamma = 0.995\n",
    "gae_lambda = 1.0\n",
    "\n",
    "def get_data_by_learner_policy(\n",
    "    env,\n",
    "    policy_net,\n",
    "    value_net,\n",
    "    discriminator_net,\n",
    "    gamma=gamma,\n",
    "    gae_lambda=gae_lambda,\n",
    "    num_samples=num_samples,\n",
    "    batch_size=128,\n",
    "):\n",
    "    \"\"\"\n",
    "    Collect samples with policy pi.\n",
    "    To speed up training, this function runs as a batch.\n",
    "\n",
    "    Only advantages (obtained by value_net) in outputs are\n",
    "    gradient-enabled tensors.\n",
    "    (All other outputs are no-gradient tensors.)\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    env : GridWorld\n",
    "        Environment class.\n",
    "    policy_net : torch.nn.Module\n",
    "        Policy network to pick up action.\n",
    "    value_net : torch.nn.Module\n",
    "        Value network used to get values and advantages.\n",
    "    discriminator_net : torch.nn.Module\n",
    "        Discriminator network used to get values and advantages\n",
    "    gamma : float\n",
    "        A discount value.\n",
    "    gae_lambda : float\n",
    "        A parameter controlling bias and variance in GAE. (See above)\n",
    "    num_samples : int\n",
    "        Number of samples to pick up.\n",
    "    batch_size : int\n",
    "        Batch size used to pick up samples.\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    states : torch.tensor((num_samples), dtype=int)\n",
    "        Collected states.\n",
    "    actions : torch.tensor((num_samples), dtype=int)\n",
    "        Collected actions.\n",
    "    logits : torch.tensor((num_samples, ACTION_SIZE), dtype=float)\n",
    "        Logits used to pick up actions.\n",
    "    advantages : torch.tensor((num_samples), dtype=float)\n",
    "        Advantages which is used to optimize policy.\n",
    "        This advantage is obtained by GAE (generalized advantage estimation).\n",
    "        This tensor has graph to be optimized (i.e, can be used for optimization.)\n",
    "    discount : torch.tensor((num_samples), dtype=float)\n",
    "        Discount factor gamma^t.\n",
    "        Later this coefficient is used to get gamma-discounted causal entropy.\n",
    "    average_reward : torch.tensor(float)\n",
    "        The average of episode's reward in all executed episodes.\n",
    "        Reward is not used in GAIL algorithm,\n",
    "        but it's used for evaluation during training in this example.\n",
    "    \"\"\"\n",
    "\n",
    "    ##########\n",
    "    # Operations are processed as a batch.\n",
    "    ##########\n",
    "\n",
    "    # initialize result list\n",
    "    states_all = torch.empty((0), dtype=int).to(device)\n",
    "    actions_all = torch.empty((0), dtype=int).to(device)\n",
    "    logits_all = torch.empty((0, ACTION_SIZE)).to(device)\n",
    "    advantages_all = torch.empty((0)).to(device)\n",
    "    discount_all = torch.empty((0)).to(device)\n",
    "\n",
    "    # note : reward is not used in GAIL,\n",
    "    #        but here we use it for evaluation during training.\n",
    "    episode_rewards = torch.empty((0)).to(device)\n",
    "\n",
    "    # flag for processing/finish in a batch\n",
    "    # (True: processing, False: finished)\n",
    "    proceed_flag = torch.zeros(batch_size, dtype=bool).to(device)\n",
    "\n",
    "    while len(states_all) < num_samples:\n",
    "        #\n",
    "        # start batch\n",
    "        # (initialize episode)\n",
    "        #\n",
    "\n",
    "        if not torch.any(proceed_flag):\n",
    "            # reset episode\n",
    "            reward_total = torch.zeros(batch_size).to(device)\n",
    "            s = env.reset(batch_size)\n",
    "            # create storage for a single episode\n",
    "            states = torch.empty((batch_size, 0), dtype=int).to(device)\n",
    "            actions = torch.empty((batch_size, 0), dtype=int).to(device)\n",
    "            logits = torch.empty((batch_size, 0, ACTION_SIZE)).to(device)\n",
    "            mask = torch.empty((batch_size, 0), dtype=bool).to(device)\n",
    "            # initialize flag\n",
    "            proceed_flag = torch.ones(batch_size, dtype=bool).to(device)\n",
    "\n",
    "        #\n",
    "        # process 1 step in episode\n",
    "        #\n",
    "\n",
    "        # expand sequence dimension\n",
    "        # (i.e., shape (batch_size, seq_len) --> (batch_size, seq_len+1))\n",
    "        states = F.pad(input=states, pad=(0, 1, 0, 0), mode=\"constant\", value=0)\n",
    "        actions = F.pad(input=actions, pad=(0, 1, 0, 0), mode=\"constant\", value=0)\n",
    "        logits = F.pad(input=logits, pad=(0, 0, 0, 1, 0, 0), mode=\"constant\", value=1.0/ACTION_SIZE)\n",
    "        mask = F.pad(input=mask, pad=(0, 1, 0, 0), mode=\"constant\", value=False)\n",
    "        # add state\n",
    "        states[proceed_flag,-1] = s\n",
    "        # add action and logits with policy pi\n",
    "        s_onehot = F.one_hot(s, num_classes=STATE_SIZE)\n",
    "        l = policy_net(s_onehot.float()).detach() # detach() - no gradient\n",
    "        probs = F.softmax(l, dim=-1)\n",
    "        a = torch.multinomial(probs, num_samples=1).squeeze(dim=-1)\n",
    "        actions[proceed_flag,-1] = a\n",
    "        logits[proceed_flag,-1,:] = l\n",
    "        # add mask\n",
    "        mask[proceed_flag,-1] = True\n",
    "        # step to the next state\n",
    "        s, r, done = env.step(a, s)\n",
    "        # set truncation flag\n",
    "        trunc = torch.tensor(env.step_count==MAX_TIMESTEP).to(device)\n",
    "        # (note : reward is only used for evaluation)\n",
    "        reward_total[proceed_flag] += r\n",
    "\n",
    "        #\n",
    "        # finalize batch\n",
    "        # (compute advantage and store a batch in results)\n",
    "        #\n",
    "        if torch.logical_or(torch.all(done), trunc):\n",
    "            seq_len = mask.shape[1]\n",
    "            # get log(D(s,a))\n",
    "            states_onehot = F.one_hot(states, num_classes=STATE_SIZE).float()\n",
    "            actions_onehot = F.one_hot(actions, num_classes=ACTION_SIZE).float()\n",
    "            state_action = torch.cat((states_onehot, actions_onehot), dim=-1)\n",
    "            d_log = torch.log(discriminator_net(state_action).detach().squeeze(dim=-1)) # detach() - no gradient\n",
    "            # get values and value loss (see above description for TD)\n",
    "            values = value_net(states_onehot).squeeze(dim=-1) # gradient tensors\n",
    "            # get next values\n",
    "            values_next = values[:,1:]\n",
    "            values_next = F.pad(input=values_next, pad=(0, 1, 0, 0), mode=\"constant\", value=0.0)\n",
    "            # if truncated, set value_net(final_state) as final next value\n",
    "            # (0 otherwise)\n",
    "            not_done = torch.logical_not(done)\n",
    "            if trunc and not_done.int().sum() > 0:\n",
    "                last_states = s[not_done]\n",
    "                last_states_onehot = F.one_hot(last_states, num_classes=STATE_SIZE).float()\n",
    "                last_values = value_net(last_states_onehot).squeeze(dim=-1) # gradient tensors\n",
    "                target_indices = torch.arange(batch_size).to(device)\n",
    "                target_indices = torch.masked_select(target_indices, proceed_flag)\n",
    "                target_indices = torch.masked_select(target_indices, not_done)\n",
    "                values_next[target_indices, -1] = last_values\n",
    "            # get delta\n",
    "            delta = d_log + values_next * gamma - values\n",
    "            # get advantages (see above for GAE)\n",
    "            gae_params = torch.tensor([(gamma * gae_lambda)**i for i in range(seq_len)]).to(device)\n",
    "            adv = [torch.sum(delta[:,i:] * gae_params[:(seq_len - i)], dim=-1) for i in range(seq_len)] # list of tensors\n",
    "            adv = torch.stack(adv, dim=1) # shape (batch_size, seq_len)\n",
    "            # get gamma-discount\n",
    "            discount = torch.tensor([gamma**i for i in range(seq_len)]).to(device)\n",
    "            discount = discount.unsqueeze(dim=0).expand(batch_size, -1)\n",
    "            # sort by game length\n",
    "            game_len = mask.sum(dim=1)\n",
    "            _, sort_indices = torch.sort(game_len)\n",
    "            mask = mask[sort_indices,:]\n",
    "            states = states[sort_indices,:]\n",
    "            actions = actions[sort_indices,:]\n",
    "            logits = logits[sort_indices,:,:]\n",
    "            adv = adv[sort_indices,:]\n",
    "            discount = discount[sort_indices,:]\n",
    "            # append to result list\n",
    "            states_all = torch.cat((states_all, torch.masked_select(states, mask)))\n",
    "            actions_all = torch.cat((actions_all, torch.masked_select(actions, mask)))\n",
    "            mask_logits = mask.unsqueeze(dim=-1).expand(batch_size, seq_len, ACTION_SIZE)\n",
    "            logits_all = torch.cat((logits_all, torch.masked_select(logits, mask_logits).reshape(-1, ACTION_SIZE)))\n",
    "            advantages_all = torch.cat((advantages_all, torch.masked_select(adv, mask)))\n",
    "            discount_all = torch.cat((discount_all, torch.masked_select(discount, mask)))\n",
    "            # append all total rewards in a batch\n",
    "            # (note : reward is only used for evaluation)\n",
    "            episode_rewards = torch.cat((episode_rewards, reward_total))\n",
    "\n",
    "        #\n",
    "        # update for next iteration\n",
    "        # (update proceed_flag and s)\n",
    "        #\n",
    "\n",
    "        batch_indices = torch.arange(batch_size).to(device)\n",
    "        process_indices = torch.masked_select(batch_indices, proceed_flag)\n",
    "        # update proceed_flag - done check\n",
    "        proceed_flag[process_indices] = torch.logical_and(proceed_flag[process_indices], torch.logical_not(done))\n",
    "        # update proceed_flag - truncation check\n",
    "        proceed_flag = torch.logical_and(proceed_flag, torch.logical_not(trunc))\n",
    "        # update s (current state)\n",
    "        s = s[torch.logical_not(torch.logical_or(done, trunc))]\n",
    "\n",
    "    # truncate results\n",
    "    states_all = states_all[:num_samples]\n",
    "    actions_all = actions_all[:num_samples]\n",
    "    logits_all = logits_all[:num_samples,:]\n",
    "    advantages_all = advantages_all[:num_samples]\n",
    "    discount_all = discount_all[:num_samples]\n",
    "    # shuffle results (because it's sorted by game length)\n",
    "    rnd_indices = torch.randperm(num_samples)\n",
    "    states_all = states_all[rnd_indices]\n",
    "    actions_all = actions_all[rnd_indices]\n",
    "    logits_all = logits_all[rnd_indices,:]\n",
    "    advantages_all = advantages_all[rnd_indices]\n",
    "    discount_all = discount_all[rnd_indices]\n",
    "    # (note : reward is only used for evaluation)\n",
    "    reward_mean = torch.mean(episode_rewards)\n",
    "\n",
    "    return states_all, actions_all, logits_all, advantages_all, discount_all, reward_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb827d02-242f-41a5-b6ef-bc68e636abf9",
   "metadata": {},
   "source": [
    "### 4. Create a function to get loss for updating $D_w$ (discriminator network)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "561b1fe5-8d41-4c13-a7cc-5940bac98082",
   "metadata": {},
   "source": [
    "Now I build a function to get loss for updating $D_w$ by maximizing $\\mathbb{E}_{\\pi} [\\log(D(s, a))] + \\mathbb{E}_{\\pi_E} [\\log(1 - D(s, a))]$.<br>\n",
    "Maximizing above equation is equivalent to minimizing the following equation. Each term can then be obtained by binary cross entropy (BCE) loss.\n",
    "\n",
    "$\\displaystyle \\verb|discriminator_loss| \\coloneqq (-\\mathbb{E}_{\\pi} [\\log(D(s, a))]) + (-\\mathbb{E}_{\\pi_E} [\\log(1 - D(s, a))])$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d4609b19-d169-4d0e-aec8-447b652e6260",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_discriminator_loss(discriminator_net, exp_states, exp_actions, pi_states, pi_actions):\n",
    "    \"\"\"\n",
    "    Collect samples with policy pi.\n",
    "    To speed up training, this function runs as batch.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    discriminator_net : torch.nn.Module\n",
    "        Discriminator network to be updated\n",
    "    exp_states : torch.tensor((num_samples), dtype=int)\n",
    "        States visited by expert policy.\n",
    "    exp_actions : torch.tensor((num_samples), dtype=int)\n",
    "        Corresponding actions to be taken by expert.\n",
    "    pi_states : torch.tensor((num_samples), dtype=int)\n",
    "        States visited by policy pi.\n",
    "    pi_actions : torch.tensor((num_samples), dtype=int)\n",
    "        Corresponding actions to be taken by policy pi.\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    Mean of discriminator loss\n",
    "    \"\"\"\n",
    "\n",
    "    # get D(s,a)\n",
    "    states_onehot_pi = F.one_hot(pi_states, num_classes=STATE_SIZE).float()\n",
    "    actions_onehot_pi = F.one_hot(pi_actions, num_classes=ACTION_SIZE).float()\n",
    "    state_action_pi = torch.cat((states_onehot_pi, actions_onehot_pi), dim=-1)\n",
    "    d_pi = discriminator_net(state_action_pi).squeeze(dim=-1)\n",
    "\n",
    "    states_onehot_exp = F.one_hot(exp_states, num_classes=STATE_SIZE).float()\n",
    "    actions_onehot_exp = F.one_hot(exp_actions, num_classes=ACTION_SIZE).float()\n",
    "    state_action_exp = torch.cat((states_onehot_exp, actions_onehot_exp), dim=-1)\n",
    "    d_exp = discriminator_net(state_action_exp).squeeze(dim=-1)\n",
    "\n",
    "    # get mean of binary cross entropy (BCE) loss\n",
    "    mean_loss_pi = F.binary_cross_entropy(d_pi, torch.ones_like(d_pi).to(device))\n",
    "    mean_loss_exp = F.binary_cross_entropy(d_exp, torch.zeros_like(d_exp).to(device))\n",
    "\n",
    "    return mean_loss_pi + mean_loss_exp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "005a908c-29d3-48e7-b054-67c6b69af572",
   "metadata": {},
   "source": [
    "### 5. Create a function to get loss for updating $\\pi_{\\theta}$ (policy network)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53fb05a6-1311-4d38-940f-9c47fa3bab56",
   "metadata": {},
   "source": [
    "Now we create a function to get loss for updating $\\theta$ to minimize $\\mathbb{E}_{\\pi} [\\log(D(s, a))] - \\lambda H(\\pi) $ (using the updated discriminator $D(s,a)$) with reinforcement learning algorithms.\n",
    "\n",
    "In [original paper](https://arxiv.org/pdf/1606.03476), TRPO (trust region policy optimization) is used to update policy parameters $\\theta$, but here **I instead use PPO** with entropy regularizer (see [here](https://github.com/tsmatz/reinforcement-learning-tutorials/blob/master/04-ppo.ipynb)) to update $\\theta$, because PPO is easier to implement. (In practical TRPO (to solve problems analytically), it needs approximation by Taylor expansion and Lagrangians.)<br>\n",
    "I note that both TRPO and PPO are motivated by the same objective to avoid stepping so far (by using KL-divergence) in optimizing policy.\n",
    "\n",
    "Now let's briefly summarize PPO algorithm.\n",
    "\n",
    "As I have discussed in [here](https://github.com/tsmatz/reinforcement-learning-tutorials/blob/master/04-ppo.ipynb), PPO algorithm optimizes $\\theta$ to minimize advantage loss, KL (KL-divergence) loss, value loss, and also entropy loss $- \\lambda H(\\pi)$. (Here $\\lambda$ is a weight's coefficient for entropy loss.)\n",
    "\n",
    "As I have discussed in [here](https://github.com/tsmatz/reinforcement-learning-tutorials/blob/master/04-ppo.ipynb), the advantage loss is obtained by the following equation (see below note) :\n",
    "\n",
    "$\\displaystyle \\mathbb{E}_{\\pi} \\left[ \\frac{P(a | \\pi_\\theta (s))}{P(a | \\pi_{\\theta_{old}} (s))} A \\right]$\n",
    "\n",
    "where $A$ is the advantage obtained by the value $\\log(D(s, a))$.\n",
    "\n",
    "> Note : In regular RL, $\\mathbb{E}(R)$ (where $R$ is reward) is maximized in optimization, but here $\\mathbb{E}(\\log(D(s,a)))$ should be minimized, because $\\log(D(s,a))$ is a loss score (which value is always negative) and $\\log(D(s,a))$ is getting lower (i.e, $D(s,a)$ is closer to 0) when it’s optimized.<br>\n",
    "> Then advantages in PPO should also be minimized (not maximized) in policy optimization in GAIL algorithm.\n",
    "\n",
    "The entropy $ H $ is $\\gamma$-discounted causal entropy as follows. (See [MCE IRL example](./04_mce_irl.ipynb).) :\n",
    "\n",
    "$\\displaystyle H(\\pi) \\coloneqq \\mathbb{E}_{\\pi} \\left[ -\\sum_t \\gamma^t \\log \\pi_t(a_t|s_t) \\right] $\n",
    "\n",
    "For value loss, I'll simply use the mean square loss (MSE) of advantages in this example. (See above description for GAE with $\\lambda_{\\verb|GAE|}=1.0$.)<br>\n",
    "Unlike advantages itself, value loss is always positive.\n",
    "\n",
    "PPO has several options and variants, and see [PPO tutorial](https://github.com/tsmatz/reinforcement-learning-tutorials/blob/master/04-ppo.ipynb) for details. (In this example, I don't use clipped surrogate objectives, and don't also use normalized advantages.)\n",
    "\n",
    "> Note : Log probability is equivalent to the negative value of cross-entropy error in categorical distribution. I have then used ```-torch.nn.functional.cross_entropy()``` to get log probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "216ac564-bbed-4277-ae45-08de2b3cedd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_policy_loss(policy_net, value_net, states, actions, logits, advantages, discount):\n",
    "    logits_old = logits\n",
    "\n",
    "    # get logits to be used for optimization\n",
    "    s_onehot = F.one_hot(states, num_classes=STATE_SIZE)\n",
    "    logits_new = policy_net(s_onehot.float())\n",
    "\n",
    "    # get advantage loss (see above)\n",
    "    logprb_old = -F.cross_entropy(logits_old, actions, reduction=\"none\") # get log probability (see above note)\n",
    "    logprb_new = -F.cross_entropy(logits_new, actions, reduction=\"none\") # get log probability (see above note)\n",
    "    prb_ratio = torch.exp(logprb_new - logprb_old) # P(a|pi_new(s)) / P(a|pi_old(s))\n",
    "    advantage_loss = prb_ratio * advantages\n",
    "\n",
    "    # get value loss (see above)\n",
    "    value_loss = 0.5 * torch.mean(advantages**2)\n",
    "\n",
    "    # get KL loss\n",
    "    # (see https://github.com/tsmatz/reinforcement-learning-tutorials/blob/master/04-ppo.ipynb)\n",
    "    l_old = logits_old - torch.amax(logits_old, dim=1, keepdim=True) # reduce quantity\n",
    "    l_new = logits_new - torch.amax(logits_new, dim=1, keepdim=True) # reduce quantity\n",
    "    e_old = torch.exp(l_old)\n",
    "    e_new = torch.exp(l_new)\n",
    "    e_sum_old = torch.sum(e_old, dim=1, keepdim=True)\n",
    "    e_sum_new = torch.sum(e_new, dim=1, keepdim=True)\n",
    "    p_old = e_old / e_sum_old\n",
    "    kl_loss = torch.sum(\n",
    "        p_old * (l_old - torch.log(e_sum_old) - l_new + torch.log(e_sum_new)),\n",
    "        dim=1,\n",
    "        keepdim=True)\n",
    "\n",
    "    # get gamma-discounted causal entropy loss (see above)\n",
    "    entropy_loss = -discount * logprb_new\n",
    "\n",
    "    return advantage_loss, value_loss, kl_loss, entropy_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45c692bb-5bed-45df-befd-a061e825cbec",
   "metadata": {},
   "source": [
    "### 6. Put it all together (Train and optimize parameter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9974426d-d429-448c-b7bc-94cea050fdfb",
   "metadata": {},
   "source": [
    "Now we alternately update $w$ (discriminator network) and $\\theta$ (policy network).\n",
    "\n",
    "In this game, the maximum episode's reward without losing any rewards is ```10.0```, and I have then trained the agent until the estimated reward is over ```5.0```. (See [Readme.md](https://github.com/tsmatz/imitation-learning-tutorials/blob/master/Readme.md) for game rule in this environment.)\n",
    "\n",
    "> Note : The reward is not used in optimization, and it's used only for evaluation. (The reward cannot be used in practices.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c23f2a80-0471-4682-abdc-88ce3c839e38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter0 - reward mean -69.0625\n",
      "iter199 - reward mean -65.7347\n",
      "iter399 - reward mean -24.1671\n",
      "iter599 - reward mean -4.0419\n",
      "iter799 - reward mean -0.1241\n",
      "iter999 - reward mean 1.52946\n",
      "iter1199 - reward mean 2.5744\n",
      "iter1399 - reward mean 3.0296\n",
      "iter1599 - reward mean 3.4640\n",
      "iter1799 - reward mean 3.6661\n",
      "iter1999 - reward mean 3.9971\n",
      "iter2199 - reward mean 4.1897\n",
      "iter2399 - reward mean 4.3314\n",
      "iter2599 - reward mean 4.4973\n",
      "iter2799 - reward mean 4.7130\n",
      "iter2999 - reward mean 4.7938\n",
      "iter3199 - reward mean 4.9675\n",
      "iter3381 - reward mean 5.1719\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "vf_coeff = 0.02\n",
    "kl_coeff = 1.0\n",
    "_lambda = 0.005\n",
    "\n",
    "reward_records = []\n",
    "\n",
    "opt_d = torch.optim.AdamW(discriminator.parameters(), lr=0.001)\n",
    "opt_pi = torch.optim.AdamW(list(policy_func.parameters()) + list(value_func.parameters()), lr=0.001)\n",
    "\n",
    "for iter_num in range(10000):\n",
    "    # get expert data\n",
    "    states_ex, actions_ex = get_data_by_expert()\n",
    "\n",
    "    # get data by policy pi\n",
    "    states, actions, logits, advantages, discount, reward_mean = get_data_by_learner_policy(\n",
    "        env=env,\n",
    "        policy_net=policy_func,\n",
    "        value_net=value_func,\n",
    "        discriminator_net=discriminator,\n",
    "    )\n",
    "    reward_records.append(reward_mean.item())\n",
    "\n",
    "    # update discriminator\n",
    "    d_loss = get_discriminator_loss(\n",
    "        discriminator,\n",
    "        states_ex,\n",
    "        actions_ex,\n",
    "        states,\n",
    "        actions,\n",
    "    )\n",
    "    opt_d.zero_grad()\n",
    "    d_loss.backward()\n",
    "    opt_d.step()\n",
    "\n",
    "    # update policy\n",
    "    adv_loss, val_loss, kl_loss, ent_loss = get_policy_loss(\n",
    "        policy_func,\n",
    "        value_func,\n",
    "        states,\n",
    "        actions,\n",
    "        logits,\n",
    "        advantages,\n",
    "        discount,\n",
    "    )\n",
    "    pi_loss = adv_loss + val_loss * vf_coeff + kl_loss * kl_coeff + ent_loss * _lambda\n",
    "    opt_pi.zero_grad()\n",
    "    pi_loss.sum().backward()\n",
    "    opt_pi.step()\n",
    "\n",
    "    # output log\n",
    "    if (iter_num == 0) or ((iter_num + 1) % 200 == 0):\n",
    "        line_end = \"\\n\"\n",
    "        print_reward = np.average(reward_records[-200:])\n",
    "    else:\n",
    "        line_end = \"\\r\"\n",
    "        print_reward = reward_records[-1]\n",
    "    print(\"iter{} - reward mean {:2.4f}\".format(iter_num, print_reward), end=line_end)\n",
    "\n",
    "    # stop if reward mean reaches to threshold\n",
    "    if np.average(reward_records[-200:]) > 5.0:\n",
    "        break\n",
    "\n",
    "print(\"\\nDone\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d69bcaa8-c14d-408c-a1f5-c6584faaf19c",
   "metadata": {},
   "source": [
    "I plot all evaluated results (the average of total reward in a episode) during training.<br>\n",
    "As you can see below, the agent is well-trained and optimized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c62d46e0-a1a7-424e-adc8-809b29d3159e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x73e5b42cfef0>]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(reward_records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ae3cf9-1a82-46f8-8632-b1746d2f9022",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
